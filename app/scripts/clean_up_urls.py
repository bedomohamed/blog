# This script will comb though the current urls.txt generated by the reddit scraper and re-write it
# omitting broken links (and eventually images that are too large for the Pi
import urllib


# Function to determine size of URL via HTML header data
def getsize(uri):
    file = urllib.urlopen(uri)
    size = file.headers.get("content-length")
    file.close()
    return int(size)

# Opening current URL file for reading
file_object = open('E:/programming/projects/blog/app/templates/pi_display/urls.txt', 'r')

# Creating list from urls.txt, closing the file
urls = list(file_object)
file_object.close()

# Opening (or creating) files to be used in the loop
clean_urls = open('E:/programming/projects/blog/app/templates/pi_display/clean_urls.txt', 'a+')
bad_urls = open('E:/programming/projects/blog/app/templates/pi_display/bad_urls.txt', 'a')

# Initializing variable to hold count of removed links, for logging
count = 0

# Loop to determine if link is broken/file too large.  URL written to either clean or bad URLs files.
for url in urls:
    # Removing newline character from URL
    end_point = url.find('\n')
    url = url[:end_point]
    try:
        # Imgur 'removed' image is 503 bytes
        if getsize(url) == 503:
            print '%s is a broken link, skipping...' % url
            bad_urls.write(str(url) + '\n')
            count += 1
        else:
            print 'Clean URL, adding...'
            clean_urls.write(str(url) + '\n')
    except (TypeError, IOError):
        print 'No length data in HTML header, adding...'
        clean_urls.write(str(url) + '\n')
        continue

# Closing files
clean_urls.close()
bad_urls.close()

# Creating Python list from newly populated clean_urls.txt
updated_clean_urls = open('E:/programming/projects/blog/app/templates/pi_display/clean_urls.txt', 'r')
clean_urls_list = list(updated_clean_urls)
updated_clean_urls.close()

# Opening/closing urls.txt (taking advantage of side effect to erase contents)
open('E:/programming/projects/blog/app/templates/pi_display/urls.txt', 'w').close()

# Re-opening urls.txt, appending with contents of the clean_urls.txt list, closing the file
url_file = open('E:/programming/projects/blog/app/templates/pi_display/urls.txt', 'a+')
for url in clean_urls_list:
    url_file.write(url)
url_file.close()

# Opening updated file, printing # of URLs
with open('E:/programming/projects/blog/app/templates/pi_display/urls.txt', 'r') as f:
    number_of_gifs = len(list(f))
    print '\nNumber of GIFS: %d' % number_of_gifs
print '%d links removed' % count

# Opening and closing the clean_urls.txt file.  Side effect to erase contents of file.
print '\n\n\n\nErasing contents of clean_urls.txt...'
open('E:/programming/projects/blog/app/templates/pi_display/clean_urls.txt', 'w').close()