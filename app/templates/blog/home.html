{% extends "/blog/base.html" %}
{% block content %}

<div id="grey">
    <div class="container">
	    <div class="row">
		    <div class="col-lg-8 col-lg-offset-2">
			    <img src="/static/images/dogsicon.png"><ba>Tyler Kershner</ba>
				<h2>Slow and Steady</h2>
                <bd>July 7th, 2014</bd>
                <hr>
				<p>
                    <b>Work continues steadily on my GIF display</b>.  I'm gradually learning that when it comes to
                    software development side projects it might not always be wise to set a deadline or ETA before starting.
                    Even though life continually gets in the way I've still managed to get some work done on
                    the project.<br><br>
                    First, and most importantly, I think I'm <em>finally</em> somewhat happy with my reddit scraping
                    code.  You can check out the completed bit on my <a href="https://github.com/kershner/blog/blob/master/app/scripts/scrape_reddit.py">Github</a>.
                    The important modifications are mostly in the logic flow of the loop through the subreddit's
                    submissions, but there's also my discovery of the <a href="https://docs.python.org/2/library/stdtypes.html#str.endswith">endswith()</a>
                    function, which makes checking for a .gif file much cleaner.  I also added an <b>if __name__ == '__main__'</b>
                    clause to make running the script easier between my work and home PCs.  It works pretty well, and I
                    also made similar modifications to my URL clean-up script, located <a href="https://github.com/kershner/blog/blob/master/app/scripts/clean_up_urls.py">here</a>.<br><br>
                    I also added a small bit of code on my server to ensure that the same GIF doesn't play twice until all
                    the GIFs from my urls.txt file have played.<br><br>
                    The basic idea is to now maintain two lists of URLs.  One is the master file that contains all my
                    scraped URLs (and gets added to when I run the scraper), and another that serves as the current
                    'playlist' of GIFs, removing the URL that has just played each time.
                </p>
<pre><code data-language="python">path = '/home/tylerkershner/app/templates/pi_display'

urls_file = open('%s/urls.txt' % path, 'r')
urls_list = list(urls_file)
urls_file.close()

urls_toplay_file = open('%s/urls_to_play.txt' % path, 'r')
urls_toplay_list = list(urls_toplay_file)
urls_toplay_file.close()

# If there are no more URLs in the to_play file, create a new one
if len(urls_toplay_list) > 1:
    pass
else:
    urls_toplay_file = open('%s/urls_to_play.txt' % path, 'a+')
    for entry in urls_list:
        urls_toplay_file.write(entry)
        urls_toplay_file.close()

urls_toplay_file = open('%s/urls_to_play.txt' % path, 'r')
urls_toplay_list = list(urls_toplay_file)
urls_toplay_file.close()

# Choose random URL from to_play list
gif_url = random.choice(urls_toplay_list)

# Opening/closing urls.txt (taking advantage of side effect to erase contents)
open('%s/urls_to_play.txt' % path, 'w').close()

# Rewrite to_play.txt without current gif URL (won't play twice)
with open('%s/urls_to_play.txt' % path, 'a+') as urls_to_play:
    for entry in urls_toplay_list:
        if entry == gif_url:
            pass
        else:
            urls_to_play.write(entry)</code></pre>
                <p>
                    As you can see 'removing' a line from a text file isn't really a concept within programming.  Instead
                    I needed to loop through my stored urls.txt file, add every URL found to a new file (urls_to_play.txt)
                    <b>unless</b> we come across the currently selected URL, in which case it is skipped leaving us with
                    a new file that is identical except for the URL that just played.  If the list of urls_to_play is
                    empty, a new one is generated using the URLs master list and the process starts over.  A little
                    simplistic, but it works just fine!<br><br>
                    Kerry and I have been scoping out some frames in some thrift stores, but so far we haven't found
                    many to be ideal for this project.  I'll probably go with an Ikea frame I saw that I'm quite sure
                    would work, so hopefully we'll get underway this week!
                </p>

			</div>
        </div><!-- /row -->
	</div> <!-- /container -->
</div>

{% endblock %}