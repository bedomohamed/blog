{% extends "/blog/base.html" %}
{% block content %}

<div id="grey">
    <div class="container pt">
		<div class="row mt">
			<div class="centered">
                <h1>GIF Picture Frame Project</h1>
                <p>
                    <b>Using the Raspberry Pi with Python</b><br>
                    - Part 2 -
                </p>
            </div>
            <div class="col-lg-8 col-lg-offset-2">
				<hr>
                <h2>Adjustments...</h2>
                <p>
                    As I mentioned in my blog post, I underestimated the Raspberry Pi's hardware capabilities.  I figured the
                    best way to go about this project was to download the GIFs I wanted to display, place them into a
                    single folder, and then use a native Linux image viewing program to run a fullscreen slideshow from
                    that directory.  Sure, I'd have to manage the size of the GIFs folder given the Pi's limited storage
                    space, but that didn't seem too unreasonable for this project.<br><br>
                    Once I got acclimated well enough with Linux I set about investigating image viewers.  I tried quite
                    a few of them.  First I tried <a href="https://wiki.archlinux.org/index.php/feh">Feh</a>, which wouldn't animate
                    the GIF, just display the first frame as a still image.  Next I tried <a href="https://wiki.gnome.org/Apps/gthumb">gThumb</a>,
                    which had the same problem.  I finally found some viewers that would animate the GIF -
                    <a href="https://wiki.gnome.org/Apps/EyeOfGnome">Eye of Gnome</a> and <a href="http://lxde.sourceforge.net/gpicview/">GPicView</a>,
                    but I soon discovered that the Pi simply did not have enough memory to natively sustain a fullscreen
                    GIF slideshow for more than a few minutes.  The images would load impossibly slow and eventually
                    the program would crash.  I thought the whole project was doomed, figuring that the Pi simply was not
                    capable of what I wanted from it.<br><br>
                    However, I didn't give up, and eventually discovered that the browser included with Raspbian,
                    <a href="http://midori-browser.org/">Midori</a>, displayed the GIFs quickly and with seemingly no
                    large impact on the Pi's memory.  I had the first step toward a solution, but it meant refactoring
                    much of the project so far.<br><br>
                </p>
                <hr>
                <h2>The Project Evolves</h2>
                <p>
                    Remember that nice, nearly perfect script that I liked so much from <a href="/piproject1">part 1</a>?
                    Time to throw (almost) all of that out and start over!<br><br>
                    With the revelation that I would need to use the Pi's web browser to display my slideshow, I realized
                    that my project just became a hybrid web application.  I could use Flask to build a <em>very</em>
                    simple website with the GIF slideshow and just have the Pi boot directly to that website.  The first
                    step was to rewrite my reddit scraper.<br><br>
                    Moving to a web application meant that I could outsource every piece of the project from the Pi, save
                    for the actual displaying of the slideshow.  This was great because now I didn't have to store any
                    files locally, and I could tweak the vast majority of the project code from my normal host provider
                    utilizing my standard workflow.  I wouldn't have to SSH into the Pi to change things, which was a
                    great relief.<br><br>
                    This also meant that my code could be simplified a bit.  Since I didn't need to store any files locally,
                    I could get rid of the <b>download_image()</b> function entirely.  I came up with a solution that works
                    much better for this particular application.<br><br>
                    First I created an empty text file - <b>urls.txt</b> - and assigned it to a Python file object:
                </p>
<pre><code data-language="python"># Opening list of URLs in read + write mode
file_object = open('/urls.txt', 'r+')
</code></pre>
                <p>
                    I then convert that file object into a Python list object:
                </p>
<pre><code data-language="python"># Converting text file to list object to more easily perform operations on it
urls = list(file_object)
</code></pre>
                <p>
                    With the file object converted into a Python list, I could go through each reddit submission (via
                    the reddit API), testing for the correct URL type and if I had already scraped the URL (using the list object),
                    and then writing the ones I wanted to keep to the file object:
                </p>
<pre><code data-language="python">for submission in submissions:
    # First 6 statments determine which URLs to skip
    if submission.url + '\n' in urls:  # Already in urls.txt
        pass
    elif '.gif' not in submission.url:  # Not a .gif file
        pass
    elif 'minus' in submission.url:  # Link to site, not GIF file
        pass
    elif 'gifsound' in submission.url:  # Link to site, not GIF file
        pass
    elif 'gifsoup' in submission.url:  # Link to site, not GIF file
        pass
    # This is a very specific URL that causes my scraper to halt
    elif 'Von_Karman' in submission.url:
        pass
    # Some imgur URLs have a ? at the end, here we write the URL up to the ?
    elif '?' in submission.url:
        print '? found in URL, snipping and adding...'
        url_snip = submission.url.find('?')
        file_object.write(submission.url[:url_snip])
        file_object.write('\n')
        count += 1
    else:
        print '%s not found in urls.txt, adding...' % submission.url
        file_object.write(submission.url)
        file_object.write('\n')
        count += 1</code></pre>
                <p>
                    Once the loop through the submissions finished, I close the file object and call my revamped <b>log()</b>
                    function to finish out the script:
                </p>
<pre><code data-language="python">def log():
    # Logs files being added and total number of GIFs to /reddit_scraper_log.txt
    time = str(datetime.now().strftime('%I:%M %p on %A, %B %d, %Y'))
    log_data = '\n\nAdded %d gifs from /r/%s at %s.' % (count, target_subreddit, time)
    number_of_gifs = '\nTotal number of GIFs: %d' % (len(urls) + count)
    with open('/reddit_scraper_log.txt', 'a') as log_file:
        log_file.write(log_data)
        log_file.write(number_of_gifs)
    print log_data
    print number_of_gifs</code></pre>
                <p>
                    I love this solution because instead of downloading each GIF I simply grab its URL and add it to a
                    text file.  This meant that not only was I saving literally gigabytes of storage space, but also
                    that the script was much faster.<br><br>
                    You can check out the full code on my GitHub, <a href="https://github.com/kershner/blog/blob/master/app/scripts/scrape_reddit.py">here</a>.
                </p>
                <hr>
                <h2>Using Flask</h2>
                <p>
                    The next stage was to build a super simple website to display the slideshow.  It had to be as
                    small and utilitarian as possible to run smoothly on the Pi's meager hardware.  Luckily,
                    with Flask all that needed to be there was a single <b>img</b> element containing a Flask variable.
                    The <b>img</b> element is wrapped in a container div in case I need to futz with its positioning when
                    displayed on the tiny LCD:
                </p>
<pre><code data-language="python"><div class="container">
    <img id="image" src="{% raw %}{{ gif_url }}{% endraw %}">
</div></code></pre>
                <p>
                    Then, back in the <b>routes.py</b> file under the Pi Display page's decorator we have the following
                    Python code:
                </p>
<pre><code data-language="python">@app.route('/pi_display')
def pi_display():
    file_object = open('/urls.txt', 'r+')
    urls = list(file_object)
    gif_url = random.choice(urls)
    file_object.close()
    return render_template("/pi_display/pi_display.html",
                           title="Raspberry PI GIF Display",
                           gif_url=gif_url)</code></pre>
                <p>
                    Here I'm opening my <b>urls.txt</b> file which was generated by the reddit scraper, converting
                    that file object into a list object, and then using the <b>random</b> module to select an image
                    URL at random.  That URL will then become the <b>gif_url</b> variable and get passed into the <b>img</b>
                    element on the HTML page.<br><br>
                    This tiny little Python script is run every time the page is reloaded, which means a completely
                    random GIF is displayed on each reload.  So then the final piece was to get the page to reload
                    automatically on a set interval to complete the slideshow effect.  I wanted to avoid using JavaScript since it had the potential to slow
                    the page down, even though there are probably hundreds of different JS solutions to this problem.<br><br>
                    I found an even simpler solution using just the meta tag on the HTML document:
                </p>
<pre><code data-language="python">http-equiv="refresh" content="20"</code></pre>
                <p>
                    This code is placed inside the <b>< meta ></b> tags, and I've set the refresh delay to 20 seconds.
                </p><br><br>
                <hr>
                <p>
                    You can check out my GIF slideshow <a href="/pi_display">here</a>.  Please keep in mind that while I only scrape SFW reddits, there are
                    around 1600 GIFs in rotation at the time of this writing, so I can't guarantee that you won't see something that will get you fired.  Just
                    a fair warning!<br><br>
                    This solution works perfectly on the Pi.  I've tested it by running the slideshow page in
                    full screen for many hours at a time.  The Pi's temperature never raises above 49 degrees (well
                    within the safe zone) and the machine never really has trouble loading the images or displaying their animations.
                    I've put my reddit scraping script onto my Python Anywhere server and have set a bash script to be called every day
                    to scrape approximately 20 different GIF-focused subreddits.  Each day my <b>urls.txt</b> file grows by around
                    100 GIFs or so.<br><br>
                    It's a great solution, and the software side of the project is complete.  Next up is the final phase:
                    packaging everything up to be placed into a frame and making it not look terrible.<br><br>
                    Thanks for reading!
                </p>

            </div>
		</div>
	</div>
</div>


{% endblock %}